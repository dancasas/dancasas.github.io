<!doctype html>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-64375-4', 'auto');
  ga('send', 'pageview');
</script>
<head><meta charset="utf-8">
	<title>SMPLitex: A Generative Model and Dataset for 3D Human Texture Estimation from Single Image
		(BMVC 2023)</title>
	<meta name="title" content="SMPLitex: A Generative Model and Dataset for 3D Human Texture Estimation from Single Image" />
	<meta name="description" content="SMPLitex: A Generative Model and Dataset for 3D Human Texture Estimation from Single Image" />
	<meta http-equiv="content-type" content="text/html; charset=UTF-8">
	<meta name="viewport" content="initial-scale=1, maximum-scale=1">

	<link rel="shortcut icon" id="favicon" href="../../favicon.png"> 
	<meta name="author" content="Dan Casas">

	<!-- JAVASCRIPT -->	
	<script type="text/javascript" src="../../js/jquery-1.11.1.min.js"></script>
	<script type="text/javascript" src="../../js/bootstrap.min.js"></script>

	<!-- CSS -->
	<link rel="stylesheet" href="../../css/bootstrap.min.css">
	<link rel="stylesheet" href="../../css/css.css">
	<link rel="stylesheet" type="text/css" href="css/project.css" media="screen">
	<link rel="stylesheet" type="text/css" media="screen" href="css/iconize.css" />

	<!-- Docs Bootstrap: getbootstrap.com -->
</head>
<!-- Els atributs del bodi son per que el menu reflexi el lloc on estas mentre fas scroll. El offset coincideix amb el padding-top del body -->

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>

<body id="dan" data-spy="scroll" data-target="#menu" data-offset="70">
	<!-- Les ID son per cada seccio, per que fagi scroll al fer click al menu, vindrien a ser links -->
	<section id="paper_title">
		<div class="container-fluid">
			<div class="cos col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 col-sm-12 col-xs-12" id="top">
				<h2><b>SMPLitex: A Generative Model and Dataset for 3D Human Texture Estimation from Single Image</b></h2>	
				<h4 style="line-height:1.65;"><a href="http://dancasas.github.io" target="_blank" >Dan Casas</a> and Marc Comino-Trinidad<br/>
				<i>British Machine Vision Conference (BMVC)</i>, 2023</h4><br/><br/>
				<!--
				<iframe style="display: block; margin: auto;" width="560" height="315" src="https://www.youtube.com/embed/o2KJoAhEGg8" frameborder="0" allowfullscreen></iframe>
				-->
				<div class='embed-container'><iframe src='https://www.youtube.com/embed/_rNrDOE7o7U?si=X2qW8f6EOmQjsU2y&cc_load_policy=1' frameborder='0' allowfullscreen></iframe></div>
			</div>
		</div>
	</section>
	<section id="abstract">
		<div class="container-fluid">
			<div class="cos col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 col-sm-12 col-xs-12" id="top">
				<h3>Abstract</h3>
				<p>We propose SMPLitex, a method for estimating and manipulating the complete 3D appearance of humans captured from a single image. SMPLitex builds upon the recently proposed generative models for 2D images, and extends their use to the 3D domain through pixel-to-surface correspondences computed on the input image. To this end, we first train a generative model for complete 3D human appearance, and then fit it into the input image by conditioning the generative model to the visible parts of subject. Furthermore, we propose a new dataset of high-quality human textures built by sampling SMPLitex conditioned on subject descriptions and images. We quantitatively and qualitatively evaluate our method in 3 publicly available datasets, demonstrating that SMPLitex significantly outperforms existing methods for human texture estimation while allowing for a wider variety of tasks such as editing, synthesis, and manipulation..</p>
			</div>
		</div>
	</section>
	<section id="files">
		<div class="container-fluid">
			<div class="cos col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 col-sm-12 col-xs-12">
				<h3>Files</h3>
				<ul>
					<li class="grid">
						<div class="griditem">
							<a href="contents/casas_BMVC2023.pdf" target="_blank" class="imageLink"><img src="contents/casas_BMVC23_thumb.jpg"></a>
							<p><a href="contents/casas_BMVC2023.pdf" target="_blank">Paper</a></p><br>
				  		</div>
					</li>
					<li class="grid">
						<div class="griditem">
							<a href="#" target="_blank" class="imageLink"><img src="contents/icons8-dataset-100.png"></a>
							<p><a href="https://github.com/dancasas/SMPLitex/tree/main#smplitex-dataset" target="_blank">Dataset</a></p><br>
				  		</div>
					</li>
					<li class="grid">
						<div class="griditem">
							<a href="#" target="_blank" class="imageLink"><img src="contents/GitHub-Mark-120px-plus.png"></a>
							<p><a href="https://github.com/dancasas/SMPLitex" target="_blank">Code and model</a></p><br>
				  		</div>
					</li>
				</ul>
				<br>
			</div>
		</div>
	</section>
<br/>
	<section id="bibtex">
		<div class="container-fluid">
			<div class="cos col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 col-sm-12 col-xs-12">
				<h3>Citation</h3>
				<pre>@inproceedings {casas2023smplitex,
    title = {{SMPLitex: A Generative Model and Dataset for 3D Human Texture Estimation from Single Image}},
    author = {Casas, Dan and Comino-Trinidad, Marc},
    booktitle = {British Machine Vision Conference (BMVC)},
    year = {2023}
}</pre>
				   
			</div>
		</div>
	</section>
	<br>
	<section id="images">
        <div class="container-fluid">
			<div class="cos col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 col-sm-12 col-xs-12">
				<h3>Description</h3>
				<p>
					From a single image where a human is partly visible, SMPLitex automatically estimates a complete 3D texture map that can be applied to SMPL body mesh sequences.
				</p>
				<video style="width: 100%; max-width: 1080px; height: auto; filter: brightness(1.0);" autoplay loop muted playsinline>
					<!--<source src="contents/BMVC-2023-cut-01_speed_enc.mp4" type="video/mp4">-->
					<source src="contents/BMVC-2023-cut-02_enc.mp4" type="video/mp4">
				</video>
				<p>
					Under the hood, SMPLitex leverages a fine-tuned latent diffusion model (LDM) trained to generate texturemaps for photorealistic SMPL avatars. Our key intuition is that, to enable the estimation of 3D human appearance from a single image, we can <i>condition</i> the synthesis of an LDM for human appearance to <i>the visible parts of the subject</i> in the input image.
				</p>
				<p>
					To this end, given an input image, we estimate pixel-to-surface correspondences and project the pixels of input image with assigned surface correspondences to a partial UV map. We then use the partial UV map as a conditional signal to sample a fine-tunned diffusion model for human texturemaps.
				</p>
				<!--
				<video style="width: 100%; max-width: 1080px; height: auto; filter: brightness(1.0);" autoplay loop muted playsinline>
					<source src="contents/BMVC-2023-cut-03_enc.mp4" type="video/mp4">
				</video>
				-->
				<h3>Results</h3>
				<p>
					Here we show results using input images from the DeepFashion-MultiModal dataset. Each triplet shows: input image, estimated texturemap, and 3D render.
				</p>
				<video style="width: 100%; max-width: 1080px; height: auto; filter: brightness(1.0);" autoplay loop muted playsinline>
					<source src="contents/BMVC-2023-cut-05-labels_enc.mp4" type="video/mp4">
				</video>
				<br/>
				<br/>
				<br/>
				<h3>SMPLitex samples</h3>
				<p>
					SMPLitex, our generative model used to estimate human textures from images, can also be sampled by text prompts. We leverage this capability to build a dataset of high-quality textures by simply sampling the latent space. Below we showcase a few of the SMPLitex samples. Notice that none of these were used to fine-tune SMPLitex.
				</p>
				<video style="width: 100%; max-width: 1080px; height: auto; filter: brightness(1.0);" autoplay loop muted playsinline>
					<source src="contents/SMPLitex-dataset-large-01_enc.mp4" type="video/mp4">
				</video>
				<video style="width: 100%; max-width: 1080px; height: auto; filter: brightness(1.0);" autoplay loop muted playsinline>
					<source src="contents/SMPLitex-dataset-large-02_enc.mp4" type="video/mp4">
				</video>
				<video style="width: 100%; max-width: 1080px; height: auto; filter: brightness(1.0);" autoplay loop muted playsinline>
					<source src="contents/SMPLitex-dataset-large-03_enc.mp4" type="video/mp4">
				</video>
				<video style="width: 100%; max-width: 1080px; height: auto; filter: brightness(1.0);" autoplay loop muted playsinline>
					<source src="contents/SMPLitex-dataset-large-04_enc.mp4" type="video/mp4">
				</video>
				<video style="width: 100%; max-width: 1080px; height: auto; filter: brightness(1.0);" autoplay loop muted playsinline>
					<source src="contents/SMPLitex-dataset-large-05_enc.mp4" type="video/mp4">
				</video>
				<video style="width: 100%; max-width: 1080px; height: auto; filter: brightness(1.0);" autoplay loop muted playsinline>
					<source src="contents/SMPLitex-dataset-large-06_enc.mp4" type="video/mp4">
				</video>
				<br/><br/>
				<h3>Comparison with state-of-the-art</h3>
				<p>
					Below we qualitative compare SMPLitex to the state-of-the-art methods for texture map estimation from single image. SMPLitex outputs higher quality textures, including face details and garment wrinkles. See <a href="contents/casas_BMVC2023.pdf">main paper</a> for quantitative analysis.
				</p>
				<video style="width: 100%; max-width: 1080px; height: auto; filter: brightness(1.0);" autoplay loop muted playsinline>
					<source src="contents/BMVC-2023-comp_03_enc.mp4" type="video/mp4">
				</video>
            </div>
		</div>
	</section>
	<br>
	<section id="Acknowledgments">
		<div class="container-fluid">
			<div class="cos col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 col-sm-12 col-xs-12">
				<h3>Acknowledgments</h3>
				This project has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement No 899739 (H2020-FETOPEN-2018-2020 CrowdDNA project).
			</div>
		</div>
	</section>
	<br>
	<section id="contact">
		<div class="container-fluid">
			<div class="cos col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 col-sm-12 col-xs-12">
				<h3>Contact</h3>
				Dan Casas – dan.casas@urjc.es
			</div>
		</div>
	</section>
	<footer>
		<div class="container-fluid">
			<div class="cos col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 col-sm-12 col-xs-12">
				<br/><br/><br/>
			</div>
		</div>
	</footer>
</body>
</html>
<script type="text/javascript">    
    /* SCROLL ALS LINKS */
    $('#menu .navbar-left li a, .navbar-brand').bind('click', function(e) {
        e.preventDefault(); // Deshabilitat la acció per defecte (que és anar al link directament, i aixi hi anem fent scroll molon)
        $('html,body').animate({scrollTop: $(this.hash).offset().top - 70}, 'slow');                                                         
    });
</script>
